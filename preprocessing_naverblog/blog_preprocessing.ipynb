{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install konlpy\n",
    "#!pip install kss --use-feature=fast-deps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sudo'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
      "��ġ ������ �ƴմϴ�.\n",
      "'sudo'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
      "��ġ ������ �ƴմϴ�.\n",
      "'rm'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
      "��ġ ������ �ƴմϴ�.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Konlpy 라이브러리 설치\n",
    "\n",
    "# 한글 폰트 설치\n",
    "!sudo apt-get install -y fonts-nanum\n",
    "!sudo fc-cache -fv\n",
    "!rm ~/.cache/matplotlib -rf\n",
    "\n",
    "# Konlpy 라이브러리 불러오기\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "# 필요한 라이브러리 불러오기\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import kss\n",
    "from konlpy.tag import Okt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv파일 데이터프레임으로 읽기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crawling_naverblog/blog_crawled.csv 파일을 \n",
    "preprocessing_naverblog/temp.py를 사용하여 맞춤법 검사 및 클렌징을 한 후\n",
    "preprocessing_naverblog/blog_clean_data.csv를 만들어서 사용함.\n",
    "colab환경에서 hanspell library가 돌아가지 않아 그렇게 진행함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 위치\n",
    "file = 'blog_clean_data.csv'\n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4000 entries, 0 to 3999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   title     4000 non-null   object\n",
      " 1   link      4000 non-null   object\n",
      " 2   contents  4000 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 93.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[헝가리 교환학생] D+117 우울은 갑자기 찾아와</td>\n",
       "      <td>https://blog.naver.com/sypark0330/222733061710</td>\n",
       "      <td>무슨 큰일이 있었던 건 아니지만 아닌가 큰일인가 하여튼 급속도로 우울해져버렸다 오...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[노르웨이 교환학생] Day+11: 뻐스투어의 즐거움 뭉크의 우울함과 첫 한식의 반가움</td>\n",
       "      <td>https://blog.naver.com/cindy7760/223319977078</td>\n",
       "      <td>오늘은 버스투어 사실 간 장소들은 잘 기억 안 난다 오슬로에서 제일 큰 공원 스키...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>캐나다 교환학생 | 9월 3주차 : 우울과 행복 그 사이 어디쯤</td>\n",
       "      <td>https://blog.naver.com/hyevely2000/222899258235</td>\n",
       "      <td>캐나다 교환학생  월 주차 에브리데이  일상 오히려 좋아에서      결제했다...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[헝가리 교환학생] 우울과 행복사이, 피크닉 4월 일상 그리고 헝가리 미용실</td>\n",
       "      <td>https://blog.naver.com/jennadiary/223443936131</td>\n",
       "      <td>지금도 런던 스탠스테드 공항에서 시간 기다리며  블로그를 살짝 건드려 보아오  런...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>일본 교환학생 | 한국 돌아가고 싶어요, 독감 걸리면 우울해지나요, 가을 타는지...</td>\n",
       "      <td>https://blog.naver.com/slsxpseh_/223326407375</td>\n",
       "      <td>月日 친구가 일하는 스타방 방문 유학생 체험학습 파티 때 도우미로 왔던 일본인 친...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              title  \\\n",
       "0                      [헝가리 교환학생] D+117 우울은 갑자기 찾아와   \n",
       "1  [노르웨이 교환학생] Day+11: 뻐스투어의 즐거움 뭉크의 우울함과 첫 한식의 반가움   \n",
       "2               캐나다 교환학생 | 9월 3주차 : 우울과 행복 그 사이 어디쯤   \n",
       "3        [헝가리 교환학생] 우울과 행복사이, 피크닉 4월 일상 그리고 헝가리 미용실   \n",
       "4   일본 교환학생 | 한국 돌아가고 싶어요, 독감 걸리면 우울해지나요, 가을 타는지...   \n",
       "\n",
       "                                              link  \\\n",
       "0   https://blog.naver.com/sypark0330/222733061710   \n",
       "1    https://blog.naver.com/cindy7760/223319977078   \n",
       "2  https://blog.naver.com/hyevely2000/222899258235   \n",
       "3   https://blog.naver.com/jennadiary/223443936131   \n",
       "4    https://blog.naver.com/slsxpseh_/223326407375   \n",
       "\n",
       "                                            contents  \n",
       "0   무슨 큰일이 있었던 건 아니지만 아닌가 큰일인가 하여튼 급속도로 우울해져버렸다 오...  \n",
       "1   오늘은 버스투어 사실 간 장소들은 잘 기억 안 난다 오슬로에서 제일 큰 공원 스키...  \n",
       "2     캐나다 교환학생  월 주차 에브리데이  일상 오히려 좋아에서      결제했다...  \n",
       "3   지금도 런던 스탠스테드 공항에서 시간 기다리며  블로그를 살짝 건드려 보아오  런...  \n",
       "4   月日 친구가 일하는 스타방 방문 유학생 체험학습 파티 때 도우미로 왔던 일본인 친...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#잘 생성됐는지 확인\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "939"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결측치 확인\n",
    "is_null = (df['contents'] == '').sum()\n",
    "is_null = is_null + (df['contents'] == ' ').sum()\n",
    "is_null = is_null + (df['contents'] == '  ').sum()\n",
    "is_null = is_null + (df['contents'] == '   ').sum()\n",
    "is_null = is_null + (df['contents'] == '    ').sum()\n",
    "is_null = is_null + (df['contents'] == '     ').sum()\n",
    "\n",
    "is_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측값 채우기: 맞춤법 검사 안 된 걸로\n",
    "\n",
    "df_temp = pd.read_csv('../crawling_naverblog/blog_crawled.csv')\n",
    "\n",
    "df['contents'] = df['contents'].replace('', pd.NA)\n",
    "df['contents'] = df['contents'].replace(' ', pd.NA)\n",
    "df['contents'] = df['contents'].replace('  ', pd.NA)\n",
    "df['contents'] = df['contents'].replace('   ', pd.NA)\n",
    "df['contents'] = df['contents'].replace('    ', pd.NA)\n",
    "df['contents'] = df['contents'].replace('     ', pd.NA)\n",
    "\n",
    "df['contents'].fillna(df_temp['contents'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리\n",
    "#### cleaned_content 구하기 (문장 불용어 제거, 전처리, 명사 남기기)\n",
    "#### filtering (우울, 불안, 스트레스 없는 문장 제거)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = 'preprocessing_blog.csv'\n",
    "\n",
    "# 필요 함수 정의\n",
    "def filter_sentences(text):\n",
    "    keywords = ['우울', '불안', '스트레스']\n",
    "    sentences = text.split('.') \n",
    "    filtered_sentences = [sentence for sentence in sentences if any(keyword in sentence for keyword in keywords)]\n",
    "    return ' '.join(filtered_sentences)\n",
    "\n",
    "def pos_filtering(text):\n",
    "    pos_list = ['Noun']\n",
    "    words = okt.pos(text, stem=True)\n",
    "    filtered_words = [word[0] for word in words if word[1] in pos_list]\n",
    "    return filtered_words\n",
    "\n",
    "def remove_stopwords(words, stopwords):\n",
    "    return [word for word in words if word not in stopwords]\n",
    "\n",
    "def preprocess_text(text, stopwords):\n",
    "    sentences = kss.split_sentences(text)\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if any(keyword in sentence for keyword in ['우울', '불안', '스트레스']):\n",
    "            filtered_words = pos_filtering(sentence)\n",
    "            cleaned_sentence = remove_stopwords(filtered_words, stopwords)\n",
    "            processed_sentences.append(cleaned_sentence)\n",
    "    return processed_sentences\n",
    "\n",
    "# 불용어 목록 로드\n",
    "with open('../stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing in batches:   0%|          | 0/1 [05:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m     subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontents\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontents\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(filter_sentences)\n\u001b[0;32m     13\u001b[0m     subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_content\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontents\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m text: re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, text))\n\u001b[1;32m---> 14\u001b[0m     subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msubset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcleaned_content\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopwords\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(subset)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 결과를 하나의 데이터프레임으로 합치기\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lovey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4760\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4632\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4633\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4634\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4635\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4636\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4751\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4752\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4753\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4754\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4758\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4760\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lovey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1207\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lovey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1287\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1281\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1287\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\lovey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lovey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2920\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[35], line 14\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     12\u001b[0m     subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontents\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontents\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(filter_sentences)\n\u001b[0;32m     13\u001b[0m     subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_content\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontents\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m text: re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, text))\n\u001b[1;32m---> 14\u001b[0m     subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_content\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m text: \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopwords\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     15\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(subset)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 결과를 하나의 데이터프레임으로 합치기\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[33], line 20\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text, stopwords)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(text, stopwords):\n\u001b[1;32m---> 20\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m \u001b[43mkss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     processed_sentences \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "File \u001b[1;32mc:\\Users\\lovey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kss\\_modules\\sentences\\split_sentences.py:88\u001b[0m, in \u001b[0;36msplit_sentences\u001b[1;34m(text, backend, num_workers, strip, return_morphemes, ignores)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m     split_fn \u001b[38;5;241m=\u001b[39m _split_sentences\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_run_job\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_analyzer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_morphemes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_morphemes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpostprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_postprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lovey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kss\\_utils\\multiprocessing.py:26\u001b[0m, in \u001b[0;36m_run_job\u001b[1;34m(func, inputs, num_workers)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_workers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m---> 26\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m         output \u001b[38;5;241m=\u001b[39m [func(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m inputs]\n",
      "File \u001b[1;32mc:\\Users\\lovey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kss\\_modules\\sentences\\split_sentences.py:135\u001b[0m, in \u001b[0;36m_split_sentences\u001b[1;34m(text, backend, strip, postprocess, recursion, return_morphemes, preprocessor, postprocessor)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    134\u001b[0m     backup_sentence \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mbackup(text)\n\u001b[1;32m--> 135\u001b[0m     morphemes \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackup_sentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m     syllables \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mpreprocess(morphemes)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], Syllable):\n",
      "File \u001b[1;32mc:\\Users\\lovey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kss\\_modules\\morphemes\\analyzers.py:64\u001b[0m, in \u001b[0;36mPecabAnalyzer.pos\u001b[1;34m(self, text, drop_space)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, drop_space: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m    Get pos information.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m        List[Tuple[str, str]]: output of analysis.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_analyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     output \u001b[38;5;241m=\u001b[39m _preserve_space(text, output, spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\v\u001b[39;00m\u001b[38;5;130;01m\\f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_space:\n",
      "File \u001b[1;32mc:\\Users\\lovey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pecab\\_pecab.py:45\u001b[0m, in \u001b[0;36mPeCab.pos\u001b[1;34m(self, text, drop_space)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, drop_space: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 45\u001b[0m     tokenization_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     47\u001b[0m         (token, pos)\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m token, pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (drop_space \u001b[38;5;129;01mand\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\f\u001b[39;00m\u001b[38;5;130;01m\\v\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\lovey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pecab\\_pecab.py:19\u001b[0m, in \u001b[0;36mPeCab._tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_tokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mset_input(text)\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     token_attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mtoken_attributes\n",
      "File \u001b[1;32mc:\\Users\\lovey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pecab\\_tokenizer.py:292\u001b[0m, in \u001b[0;36mTokenizer.increment_token\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend:\n\u001b[0;32m    291\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 292\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpending\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m    295\u001b[0m length \u001b[38;5;241m=\u001b[39m token\u001b[38;5;241m.\u001b[39mlength\n",
      "File \u001b[1;32mc:\\Users\\lovey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pecab\\_tokenizer.py:384\u001b[0m, in \u001b[0;36mTokenizer.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_entries \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data_entry \u001b[38;5;129;01min\u001b[39;00m data_entries\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 384\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m            \u001b[49m\u001b[43msurface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpos_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpos_ahead\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m            \u001b[49m\u001b[43mType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKNOWN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m         any_matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    394\u001b[0m pos_ahead \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\lovey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pecab\\_tokenizer.py:275\u001b[0m, in \u001b[0;36mTokenizer.add\u001b[1;34m(self, surface, data_dict, from_pos_data, word_pos, end_pos, type_)\u001b[0m\n\u001b[0;32m    272\u001b[0m         least_idx \u001b[38;5;241m=\u001b[39m idx\n\u001b[0;32m    274\u001b[0m least_cost \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m word_cost\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_pos\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    276\u001b[0m     cost\u001b[38;5;241m=\u001b[39mleast_cost,\n\u001b[0;32m    277\u001b[0m     last_right_id\u001b[38;5;241m=\u001b[39mright_id,\n\u001b[0;32m    278\u001b[0m     back_pos\u001b[38;5;241m=\u001b[39mfrom_pos_data\u001b[38;5;241m.\u001b[39mpos,\n\u001b[0;32m    279\u001b[0m     back_rpos\u001b[38;5;241m=\u001b[39mword_pos,\n\u001b[0;32m    280\u001b[0m     back_index\u001b[38;5;241m=\u001b[39mleast_idx,\n\u001b[0;32m    281\u001b[0m     back_id\u001b[38;5;241m=\u001b[39mword_id,\n\u001b[0;32m    282\u001b[0m     back_dict_type\u001b[38;5;241m=\u001b[39mtype_,\n\u001b[0;32m    283\u001b[0m     back_pos_type\u001b[38;5;241m=\u001b[39mback_pos_type,\n\u001b[0;32m    284\u001b[0m     morphemes\u001b[38;5;241m=\u001b[39mmorphemes,\n\u001b[0;32m    285\u001b[0m     back_pos_tag\u001b[38;5;241m=\u001b[39mleft_pos,\n\u001b[0;32m    286\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lovey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pecab\\_tokenizer.py:179\u001b[0m, in \u001b[0;36mTokenizer.WrappedPositionArray.get\u001b[1;34m(self, pos)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_positions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount):\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_positions\u001b[38;5;241m.\u001b[39mappend(\u001b[43mTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPosition\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_positions[\n\u001b[0;32m    182\u001b[0m     : \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositions) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_write\n\u001b[0;32m    183\u001b[0m ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositions[\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_write : \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositions) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_write\n\u001b[0;32m    185\u001b[0m ]\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_positions[\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositions) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_write : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_write\n\u001b[0;32m    188\u001b[0m ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositions[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_write]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 다음 1000개의 데이터를 처리하고 싶은 경우, range의 시작점을 변경\n",
    "start_index = 0  # 이전에 처리한 마지막 인덱스 + 1\n",
    "batch_size = 4000  # 한 번에 처리할 데이터의 개수\n",
    "\n",
    "file_path = 'blog_preprocessing.csv'\n",
    "\n",
    "# 데이터프레임을 지정된 범위에서 분할하여 처리\n",
    "results = []\n",
    "for start in tqdm(range(start_index, start_index + batch_size, batch_size), desc=\"Processing in batches\"):\n",
    "    end = start + batch_size\n",
    "    subset = df.iloc[start:end]  # batch_size만큼의 행을 선택\n",
    "    subset['contents'] = subset['contents'].apply(filter_sentences)\n",
    "    subset['preprocessed'] = subset['cleaned_content'].apply(lambda text: preprocess_text(text, stopwords))\n",
    "    results.append(subset)\n",
    "\n",
    "# 결과를 하나의 데이터프레임으로 합치기\n",
    "final_df = pd.concat(results)\n",
    "\n",
    "# 파일 존재 여부 확인 후 적절한 헤더 설정 및 저장\n",
    "if os.path.exists(file_path):\n",
    "    header_exists = True\n",
    "else:\n",
    "    header_exists = False\n",
    "\n",
    "final_df.to_csv(file_path, mode='a', index=False, header=not header_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
